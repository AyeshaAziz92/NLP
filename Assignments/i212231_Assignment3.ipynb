{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#    **Assignment 3**\n",
        "\n",
        "# **Name: Ayesha Aziz**                                                                   \n",
        "# **Roll Number: i21-2231**"
      ],
      "metadata": {
        "id": "u5j8TfxPTyCf"
      },
      "id": "u5j8TfxPTyCf"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKZ2ma8Q6ixm",
        "outputId": "e27ec31a-153f-4d3b-d685-8dc3b5b56243"
      },
      "id": "KKZ2ma8Q6ixm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc454238",
      "metadata": {
        "id": "cc454238"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "from scipy.special import softmax\n",
        "import matplotlib as plt\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be429bb8",
      "metadata": {
        "id": "be429bb8"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c333e8",
      "metadata": {
        "id": "18c333e8"
      },
      "source": [
        "#  Read input from a text file & Pre-processing of data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5262a660",
      "metadata": {
        "id": "5262a660"
      },
      "outputs": [],
      "source": [
        "def get_file_data(stop_word_removal='no'):\n",
        "    file_contents = []\n",
        "    with open('test.txt') as f:\n",
        "        file_contents = f.read()\n",
        "    text = []\n",
        "    for val in file_contents.split('.'):\n",
        "        sent = re.findall(\"[A-Za-z]+\", val)\n",
        "        line = ''\n",
        "        for words in sent:\n",
        "            \n",
        "            if stop_word_removal == 'yes': \n",
        "                if len(words) > 1 and words not in stop_words:\n",
        "                    line = line + ' ' + words\n",
        "            else:\n",
        "                if len(words) > 1 :\n",
        "                    line = line + ' ' + words\n",
        "        text.append(line)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a03e9c",
      "metadata": {
        "id": "43a03e9c"
      },
      "outputs": [],
      "source": [
        " def generate_dictinoary_data(text):\n",
        "    word_to_index= dict()\n",
        "    index_to_word = dict()\n",
        "    corpus = []\n",
        "    count = 0\n",
        "    vocab_size = 0\n",
        "    \n",
        "    for row in text:\n",
        "        for word in row.split():\n",
        "            word = word.lower()\n",
        "            corpus.append(word)\n",
        "            if word_to_index.get(word) == None:\n",
        "                word_to_index.update ( {word : count})\n",
        "                index_to_word.update ( {count : word })\n",
        "                count  += 1\n",
        "    vocab_size = len(word_to_index)\n",
        "    length_of_corpus = len(corpus)\n",
        "    \n",
        "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One-hot vector encoding to preprocess categorical features**"
      ],
      "metadata": {
        "id": "njH1OoYkUvyF"
      },
      "id": "njH1OoYkUvyF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fd32d6",
      "metadata": {
        "id": "22fd32d6"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
        "    \n",
        "    #Create an array of size = vocab_size filled with zeros\n",
        "    trgt_word_vector = np.zeros(vocab_size)\n",
        "    \n",
        "    #Get the index of the target_word according to the dictionary word_to_index. \n",
        "    #If target_word = best, the index according to the dictionary word_to_index is 0. \n",
        "    #So the one hot vector will be [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    index_of_word_dictionary = word_to_index.get(target_word) \n",
        "    \n",
        "    #Set the index to 1\n",
        "    trgt_word_vector[index_of_word_dictionary] = 1\n",
        "    \n",
        "    #Repeat same steps for context_words but in a loop\n",
        "    ctxt_word_vector = np.zeros(vocab_size)\n",
        "    \n",
        "    \n",
        "    for word in context_words:\n",
        "        index_of_word_dictionary = word_to_index.get(word) \n",
        "        ctxt_word_vector[index_of_word_dictionary] = 1\n",
        "        \n",
        "    return trgt_word_vector,ctxt_word_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Data**"
      ],
      "metadata": {
        "id": "GWd2Ep4yVDIU"
      },
      "id": "GWd2Ep4yVDIU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ebe270",
      "metadata": {
        "id": "f9ebe270"
      },
      "outputs": [],
      "source": [
        "#Note : Below comments for trgt_word_index, ctxt_word_index are with the above sample text for understanding the code flow\n",
        "\n",
        "def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,sample=None):\n",
        "\n",
        "    training_data =  []\n",
        "    training_sample_words =  []\n",
        "    for i,word in enumerate(corpus):\n",
        "\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "\n",
        "        #when target word is the first word\n",
        "        if i == 0:  \n",
        "\n",
        "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "            context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] \n",
        "\n",
        "\n",
        "        #when target word is the last word\n",
        "        elif i == len(corpus)-1:\n",
        "\n",
        "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "            context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size  , -1 )]\n",
        "\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "\n",
        "            #Before the middle target word\n",
        "            before_target_word_index = index_target_word - 1\n",
        "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
        "                if x >=0:\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "            #After the middle target word\n",
        "            after_target_word_index = index_target_word + 1\n",
        "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "                if x < len(corpus):\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "\n",
        "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
        "        training_data.append([trgt_word_vector,ctxt_word_vector])   \n",
        "        \n",
        "        if sample is not None:\n",
        "            training_sample_words.append([target_word,context_words])   \n",
        "        \n",
        "    return training_data,training_sample_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward propagation**"
      ],
      "metadata": {
        "id": "_2CfrrTIVSaN"
      },
      "id": "_2CfrrTIVSaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df129e4d",
      "metadata": {
        "id": "df129e4d"
      },
      "outputs": [],
      "source": [
        "def forward_prop(weight_inp_hidden,weight_hidden_output,target_word_vector):\n",
        "    \n",
        "    #target_word_vector = x , weight_inp_hidden =  weights for input layer to hidden layer  \n",
        "    hidden_layer = np.dot(weight_inp_hidden.T, target_word_vector)\n",
        "    \n",
        "    #weight_hidden_output = weights for hidden layer to output layer\n",
        "    u = np.dot(weight_hidden_output.T, hidden_layer)\n",
        "    \n",
        "    y_predicted = softmax(u)\n",
        "    \n",
        "    return y_predicted, hidden_layer, u"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backward propagation**"
      ],
      "metadata": {
        "id": "6BqqewqXVY2t"
      },
      "id": "6BqqewqXVY2t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bf31b3",
      "metadata": {
        "id": "81bf31b3"
      },
      "outputs": [],
      "source": [
        "def backward_prop(weight_inp_hidden,weight_hidden_output,total_error, hidden_layer, target_word_vector,learning_rate):\n",
        "    \n",
        "    dl_weight_inp_hidden = np.outer(target_word_vector, np.dot(weight_hidden_output, total_error.T))\n",
        "    dl_weight_hidden_output = np.outer(hidden_layer, total_error)\n",
        "    \n",
        "    # Update weights\n",
        "    weight_inp_hidden = weight_inp_hidden - (learning_rate * dl_weight_inp_hidden)\n",
        "    weight_hidden_output = weight_hidden_output - (learning_rate * dl_weight_hidden_output)\n",
        "    \n",
        "    return weight_inp_hidden,weight_hidden_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating error**"
      ],
      "metadata": {
        "id": "7Dg3EYg2Vgm8"
      },
      "id": "7Dg3EYg2Vgm8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ef0608",
      "metadata": {
        "id": "c3ef0608"
      },
      "outputs": [],
      "source": [
        "def calculate_error(y_pred,context_words):\n",
        "    \n",
        "    total_error = [None] * len(y_pred)\n",
        "    index_of_1_in_context_words = {}\n",
        "    \n",
        "    for index in np.where(context_words == 1)[0]:\n",
        "        index_of_1_in_context_words.update ( {index : 'yes'} )\n",
        "        \n",
        "    number_of_1_in_context_vector = len(index_of_1_in_context_words)\n",
        "    \n",
        "    for i,value in enumerate(y_pred):\n",
        "        \n",
        "        if index_of_1_in_context_words.get(i) != None:\n",
        "            total_error[i]= (value-1) + ( (number_of_1_in_context_vector -1) * value)\n",
        "        else:\n",
        "            total_error[i]= (number_of_1_in_context_vector * value)\n",
        "            \n",
        "            \n",
        "    return  np.array(total_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363a10ca",
      "metadata": {
        "id": "363a10ca"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(u,ctx):\n",
        "    \n",
        "    sum_1 = 0\n",
        "    for index in np.where(ctx==1)[0]:\n",
        "        sum_1 = sum_1 + u[index]\n",
        "    \n",
        "    sum_1 = -sum_1\n",
        "    sum_2 = len(np.where(ctx==1)[0]) * np.log(np.sum(np.exp(u)))\n",
        "    \n",
        "    total_loss = sum_1 + sum_2\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9fcec7",
      "metadata": {
        "id": "cb9fcec7"
      },
      "outputs": [],
      "source": [
        "def train(word_embedding_dimension,window_size,epochs,training_data,learning_rate,disp = 'no',interval=-1):\n",
        "    \n",
        "    weights_input_hidden = np.random.uniform(-1, 1, (vocab_size, word_embedding_dimension))\n",
        "    weights_hidden_output = np.random.uniform(-1, 1, (word_embedding_dimension, vocab_size))\n",
        "    \n",
        "    \n",
        "    #For analysis purposes\n",
        "    epoch_loss = []\n",
        "    weights_1 = []\n",
        "    weights_2 = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "\n",
        "        for target,context in training_data:\n",
        "            y_pred, hidden_layer, u = forward_prop(weights_input_hidden,weights_hidden_output,target)\n",
        "\n",
        "            total_error = calculate_error(y_pred, context)\n",
        "\n",
        "            weights_input_hidden,weights_hidden_output = backward_prop(\n",
        "                weights_input_hidden,weights_hidden_output ,total_error, hidden_layer, target,learning_rate\n",
        "            )\n",
        "\n",
        "            loss_temp = calculate_loss(u,context)\n",
        "            loss += loss_temp\n",
        "        \n",
        "        epoch_loss.append( loss )\n",
        "        weights_1.append(weights_input_hidden)\n",
        "        weights_2.append(weights_hidden_output)\n",
        "        \n",
        "        if disp == 'yes':\n",
        "            if epoch ==0 or epoch % interval ==0 or epoch == epochs -1:\n",
        "                print('Epoch: %s. Loss:%s' %(epoch,loss))\n",
        "    return epoch_loss,np.array(weights_1),np.array(weights_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cosine similarity**"
      ],
      "metadata": {
        "id": "F3DCeYOlVx1B"
      },
      "id": "F3DCeYOlVx1B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e936abc7",
      "metadata": {
        "id": "e936abc7"
      },
      "outputs": [],
      "source": [
        "# Input vector, returns nearest word(s)\n",
        "def cosine_similarity(word,weight,word_to_index,vocab_size,index_to_word):\n",
        "    \n",
        "    #Get the index of the word from the dictionary\n",
        "    index = word_to_index[word]\n",
        "    \n",
        "    #Get the correspondin weights for the word\n",
        "    word_vector_1 = weight[index]\n",
        "    \n",
        "    \n",
        "    word_similarity = {}\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        \n",
        "        word_vector_2 = weight[i]\n",
        "        \n",
        "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
        "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
        "        theta = theta_sum / theta_den\n",
        "        \n",
        "        word = index_to_word[i]\n",
        "        word_similarity[word] = theta\n",
        "    \n",
        "    return word_similarity #words_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf65fa70",
      "metadata": {
        "id": "bf65fa70"
      },
      "outputs": [],
      "source": [
        "def print_similar_words(top_n_words,weight,msg,words_subset):\n",
        "    \n",
        "    columns=[]\n",
        "    \n",
        "    for i in range(0,len(words_subset)):\n",
        "        columns.append('similar:' +str(i+1) )\n",
        "        \n",
        "    df = pd.DataFrame(columns=columns,index=words_subset)\n",
        "    df.head()\n",
        "    \n",
        "    row = 0\n",
        "    for word in words_subset:\n",
        "        \n",
        "        #Get the similarity matrix for the word: word\n",
        "        similarity_matrix = cosine_similarity(word,weight,word_to_index,vocab_size,index_to_word)\n",
        "        col = 0\n",
        "        \n",
        "        #Sort the top_n_words\n",
        "        words_sorted = dict(sorted(similarity_matrix.items(), key=lambda x: x[1], reverse=True)[1:top_n_words+1])\n",
        "        \n",
        "        #Create a dataframe to display the similarity matrix\n",
        "        for similar_word,similarity_value in words_sorted.items():\n",
        "            df.iloc[row][col] = (similar_word,round(similarity_value,2))\n",
        "            col += 1\n",
        "        row += 1\n",
        "    styles = [dict(selector='caption', \n",
        "    props=[('text-align', 'center'),('font-size', '20px'),('color', 'red')])] \n",
        "    df = df.style.set_properties(**\n",
        "                       {'color': 'green','border-color': 'blue','font-size':'14px'}\n",
        "                      ).set_table_styles(styles).set_caption(msg)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "WOb4x-zOWZ9j",
        "outputId": "7feda5a7-00ec-4fe2-c98b-412d87948abb"
      },
      "id": "WOb4x-zOWZ9j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1e89b8fd90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_8d2f7_ caption {\n",
              "  text-align: center;\n",
              "  font-size: 20px;\n",
              "  color: red;\n",
              "}\n",
              "#T_8d2f7_row0_col0, #T_8d2f7_row0_col1, #T_8d2f7_row0_col2, #T_8d2f7_row0_col3, #T_8d2f7_row0_col4, #T_8d2f7_row1_col0, #T_8d2f7_row1_col1, #T_8d2f7_row1_col2, #T_8d2f7_row1_col3, #T_8d2f7_row1_col4, #T_8d2f7_row2_col0, #T_8d2f7_row2_col1, #T_8d2f7_row2_col2, #T_8d2f7_row2_col3, #T_8d2f7_row2_col4, #T_8d2f7_row3_col0, #T_8d2f7_row3_col1, #T_8d2f7_row3_col2, #T_8d2f7_row3_col3, #T_8d2f7_row3_col4, #T_8d2f7_row4_col0, #T_8d2f7_row4_col1, #T_8d2f7_row4_col2, #T_8d2f7_row4_col3, #T_8d2f7_row4_col4 {\n",
              "  color: green;\n",
              "  border-color: blue;\n",
              "  font-size: 14px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_8d2f7_\" class=\"dataframe\">\n",
              "  <caption>sim_matrix for : Stopwords_removed_dimension_50_epochs_200_window_size_4</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >similar:1</th>\n",
              "      <th class=\"col_heading level0 col1\" >similar:2</th>\n",
              "      <th class=\"col_heading level0 col2\" >similar:3</th>\n",
              "      <th class=\"col_heading level0 col3\" >similar:4</th>\n",
              "      <th class=\"col_heading level0 col4\" >similar:5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8d2f7_level0_row0\" class=\"row_heading level0 row0\" >today</th>\n",
              "      <td id=\"T_8d2f7_row0_col0\" class=\"data row0 col0\" >('fundamentals', 0.36)</td>\n",
              "      <td id=\"T_8d2f7_row0_col1\" class=\"data row0 col1\" >('artificial', 0.28)</td>\n",
              "      <td id=\"T_8d2f7_row0_col2\" class=\"data row0 col2\" >('learning', 0.27)</td>\n",
              "      <td id=\"T_8d2f7_row0_col3\" class=\"data row0 col3\" >('got', 0.26)</td>\n",
              "      <td id=\"T_8d2f7_row0_col4\" class=\"data row0 col4\" >('excited', 0.26)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8d2f7_level0_row1\" class=\"row_heading level0 row1\" >use</th>\n",
              "      <td id=\"T_8d2f7_row1_col0\" class=\"data row1 col0\" >('mine', 0.32)</td>\n",
              "      <td id=\"T_8d2f7_row1_col1\" class=\"data row1 col1\" >('really', 0.29)</td>\n",
              "      <td id=\"T_8d2f7_row1_col2\" class=\"data row1 col2\" >('listening', 0.27)</td>\n",
              "      <td id=\"T_8d2f7_row1_col3\" class=\"data row1 col3\" >('time', 0.27)</td>\n",
              "      <td id=\"T_8d2f7_row1_col4\" class=\"data row1 col4\" >('studying', 0.26)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8d2f7_level0_row2\" class=\"row_heading level0 row2\" >listening</th>\n",
              "      <td id=\"T_8d2f7_row2_col0\" class=\"data row2 col0\" >('talk', 0.28)</td>\n",
              "      <td id=\"T_8d2f7_row2_col1\" class=\"data row2 col1\" >('use', 0.27)</td>\n",
              "      <td id=\"T_8d2f7_row2_col2\" class=\"data row2 col2\" >('thanks', 0.25)</td>\n",
              "      <td id=\"T_8d2f7_row2_col3\" class=\"data row2 col3\" >('time', 0.2)</td>\n",
              "      <td id=\"T_8d2f7_row2_col4\" class=\"data row2 col4\" >('intelligence', 0.19)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8d2f7_level0_row3\" class=\"row_heading level0 row3\" >alternative</th>\n",
              "      <td id=\"T_8d2f7_row3_col0\" class=\"data row3 col0\" >('artificial', 0.26)</td>\n",
              "      <td id=\"T_8d2f7_row3_col1\" class=\"data row3 col1\" >('fundamentals', 0.24)</td>\n",
              "      <td id=\"T_8d2f7_row3_col2\" class=\"data row3 col2\" >('etc', 0.22)</td>\n",
              "      <td id=\"T_8d2f7_row3_col3\" class=\"data row3 col3\" >('learning', 0.21)</td>\n",
              "      <td id=\"T_8d2f7_row3_col4\" class=\"data row3 col4\" >('names', 0.2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8d2f7_level0_row4\" class=\"row_heading level0 row4\" >became</th>\n",
              "      <td id=\"T_8d2f7_row4_col0\" class=\"data row4 col0\" >('better', 0.42)</td>\n",
              "      <td id=\"T_8d2f7_row4_col1\" class=\"data row4 col1\" >('passions', 0.36)</td>\n",
              "      <td id=\"T_8d2f7_row4_col2\" class=\"data row4 col2\" >('got', 0.31)</td>\n",
              "      <td id=\"T_8d2f7_row4_col3\" class=\"data row4 col3\" >('growing', 0.22)</td>\n",
              "      <td id=\"T_8d2f7_row4_col4\" class=\"data row4 col4\" >('studying', 0.22)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8350e515",
      "metadata": {
        "id": "8350e515"
      },
      "outputs": [],
      "source": [
        "window_size = 2\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "text = ['Thanks for listening to me talk about data science and statistics']\n",
        "\n",
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictinoary_data(text)\n",
        "training_data,training_sample_words = generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95244ae2",
      "metadata": {
        "id": "95244ae2"
      },
      "outputs": [],
      "source": [
        "dimensions = [5,10,15,20]\n",
        "loss_epoch = {}\n",
        "# fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(10,10),)\n",
        "# fig.suptitle(\"Plots for showing paramaters with varying dimension\", fontsize=16)\n",
        "row=0\n",
        "col=0\n",
        "for dim in dimensions:\n",
        "    \n",
        "    epoch_loss,weights_1,weights_2 = train(dim,window_size,epochs,training_data,learning_rate)\n",
        "    loss_epoch.update( {dim: epoch_loss} )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04aff7fa",
      "metadata": {
        "id": "04aff7fa"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "top_n_words = 5\n",
        "dimension = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbe208b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fbe208b",
        "outputId": "07ecc210-6be4-4ec5-de5f-eaafb28fdaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 32\n",
            "Length of corpus : 64\n",
            "['learning' 'became' 'thanks' 'use' 'statistics']\n"
          ]
        }
      ],
      "source": [
        "text = get_file_data('yes')\n",
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictinoary_data(text)\n",
        "training_data,training_sample_words = generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus)\n",
        "print('Number of unique words:' , vocab_size)\n",
        "print('Length of corpus :',length_of_corpus)\n",
        "words_subset = []\n",
        "words_subset = np.random.choice(list(word_to_index.keys()),top_n_words)\n",
        "print(words_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75fdf5e",
      "metadata": {
        "id": "b75fdf5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014c5fa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "014c5fa9",
        "outputId": "d714becc-86ba-4310-df30-c4a0bbd7e539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss:1257.3085821038082\n",
            "Epoch: 50. Loss:421.54808285331933\n",
            "Epoch: 100. Loss:415.0232572572148\n",
            "Epoch: 150. Loss:413.3723891978609\n",
            "Epoch: 199. Loss:412.5936457994844\n"
          ]
        }
      ],
      "source": [
        "loss_epoch = {}\n",
        "dataframe_sim = []\n",
        "\n",
        "epoch_loss,weights_1,weights_2 = train(dimension,window_size,epochs,training_data,learning_rate,'yes',50)\n",
        "loss_epoch.update( {'yes': epoch_loss} )\n",
        "\n",
        "df = print_similar_words(\n",
        "    top_n_words,\n",
        "    weights_1[epochs - 1],\n",
        "    'sim_matrix for : Stopwords_removed_dimension_' + str(dimension) + '_epochs_' + str(epochs) + '_window_size_' +str(window_size),\n",
        "    words_subset\n",
        ")\n",
        "dataframe_sim.append(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b740618",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "5b740618",
        "outputId": "7b0fe7fd-5ccf-4c13-ec4f-6e40deb53784"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1e899aed10>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ec5c3_ caption {\n",
              "  text-align: center;\n",
              "  font-size: 20px;\n",
              "  color: red;\n",
              "}\n",
              "#T_ec5c3_row0_col0, #T_ec5c3_row0_col1, #T_ec5c3_row0_col2, #T_ec5c3_row0_col3, #T_ec5c3_row0_col4, #T_ec5c3_row1_col0, #T_ec5c3_row1_col1, #T_ec5c3_row1_col2, #T_ec5c3_row1_col3, #T_ec5c3_row1_col4, #T_ec5c3_row2_col0, #T_ec5c3_row2_col1, #T_ec5c3_row2_col2, #T_ec5c3_row2_col3, #T_ec5c3_row2_col4, #T_ec5c3_row3_col0, #T_ec5c3_row3_col1, #T_ec5c3_row3_col2, #T_ec5c3_row3_col3, #T_ec5c3_row3_col4, #T_ec5c3_row4_col0, #T_ec5c3_row4_col1, #T_ec5c3_row4_col2, #T_ec5c3_row4_col3, #T_ec5c3_row4_col4 {\n",
              "  color: green;\n",
              "  border-color: blue;\n",
              "  font-size: 14px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ec5c3_\" class=\"dataframe\">\n",
              "  <caption>sim_matrix for : Stopwords_removed_dimension_50_epochs_200_window_size_2</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >similar:1</th>\n",
              "      <th class=\"col_heading level0 col1\" >similar:2</th>\n",
              "      <th class=\"col_heading level0 col2\" >similar:3</th>\n",
              "      <th class=\"col_heading level0 col3\" >similar:4</th>\n",
              "      <th class=\"col_heading level0 col4\" >similar:5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ec5c3_level0_row0\" class=\"row_heading level0 row0\" >learning</th>\n",
              "      <td id=\"T_ec5c3_row0_col0\" class=\"data row0 col0\" >('intelligence', 0.3)</td>\n",
              "      <td id=\"T_ec5c3_row0_col1\" class=\"data row0 col1\" >('alternative', 0.28)</td>\n",
              "      <td id=\"T_ec5c3_row0_col2\" class=\"data row0 col2\" >('hot', 0.25)</td>\n",
              "      <td id=\"T_ec5c3_row0_col3\" class=\"data row0 col3\" >('listening', 0.19)</td>\n",
              "      <td id=\"T_ec5c3_row0_col4\" class=\"data row0 col4\" >('big', 0.14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ec5c3_level0_row1\" class=\"row_heading level0 row1\" >became</th>\n",
              "      <td id=\"T_ec5c3_row1_col0\" class=\"data row1 col0\" >('good', 0.26)</td>\n",
              "      <td id=\"T_ec5c3_row1_col1\" class=\"data row1 col1\" >('big', 0.21)</td>\n",
              "      <td id=\"T_ec5c3_row1_col2\" class=\"data row1 col2\" >('time', 0.2)</td>\n",
              "      <td id=\"T_ec5c3_row1_col3\" class=\"data row1 col3\" >('long', 0.17)</td>\n",
              "      <td id=\"T_ec5c3_row1_col4\" class=\"data row1 col4\" >('science', 0.16)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ec5c3_level0_row2\" class=\"row_heading level0 row2\" >thanks</th>\n",
              "      <td id=\"T_ec5c3_row2_col0\" class=\"data row2 col0\" >('really', 0.2)</td>\n",
              "      <td id=\"T_ec5c3_row2_col1\" class=\"data row2 col1\" >('science', 0.18)</td>\n",
              "      <td id=\"T_ec5c3_row2_col2\" class=\"data row2 col2\" >('machine', 0.16)</td>\n",
              "      <td id=\"T_ec5c3_row2_col3\" class=\"data row2 col3\" >('data', 0.15)</td>\n",
              "      <td id=\"T_ec5c3_row2_col4\" class=\"data row2 col4\" >('became', 0.15)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ec5c3_level0_row3\" class=\"row_heading level0 row3\" >use</th>\n",
              "      <td id=\"T_ec5c3_row3_col0\" class=\"data row3 col0\" >('good', 0.27)</td>\n",
              "      <td id=\"T_ec5c3_row3_col1\" class=\"data row3 col1\" >('mine', 0.24)</td>\n",
              "      <td id=\"T_ec5c3_row3_col2\" class=\"data row3 col2\" >('long', 0.19)</td>\n",
              "      <td id=\"T_ec5c3_row3_col3\" class=\"data row3 col3\" >('better', 0.14)</td>\n",
              "      <td id=\"T_ec5c3_row3_col4\" class=\"data row3 col4\" >('studying', 0.13)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ec5c3_level0_row4\" class=\"row_heading level0 row4\" >statistics</th>\n",
              "      <td id=\"T_ec5c3_row4_col0\" class=\"data row4 col0\" >('talk', 0.32)</td>\n",
              "      <td id=\"T_ec5c3_row4_col1\" class=\"data row4 col1\" >('really', 0.15)</td>\n",
              "      <td id=\"T_ec5c3_row4_col2\" class=\"data row4 col2\" >('fundamentals', 0.15)</td>\n",
              "      <td id=\"T_ec5c3_row4_col3\" class=\"data row4 col3\" >('etc', 0.15)</td>\n",
              "      <td id=\"T_ec5c3_row4_col4\" class=\"data row4 col4\" >('mine', 0.14)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(len(dataframe_sim)):\n",
        "    display(dataframe_sim[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391da5e1",
      "metadata": {
        "id": "391da5e1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "i212231_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}